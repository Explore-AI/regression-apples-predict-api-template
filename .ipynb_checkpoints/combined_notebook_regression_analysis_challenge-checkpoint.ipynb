{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDSA Apple Prices Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine you are in the Fresh Produce Industry. How much stock do you have on hand? Not too little that you run out of stock when customers want to buy more. And not too much that food waste occurs. How do you set your prices? Yields from farms fluctuate by season. Should your prices then also fluctuate by season?\n",
    "\n",
    "With this context, EDSA is challenging you to construct a regression algorithm, capable of accurately predicting how much a kilogram of Golden Delicious Apples will cost, given certain parameters.\n",
    "\n",
    "Providing an accurate and robust solution to this challenge has immense economic potential, with users of the system being able to accurately determine how much stock to have on hand and what price to set the Golden Delicious Apples to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset consists of over 60 000 sales made by [Durban Fresh Produce Market](http://www.durban.gov.za/Online_Tools/Pages/Fresh_Produce_Price_Reporting.aspx). In this notebook, we use this data to predict the average price per kilogram of Golden Delicious Apples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a brief description for each data field:\n",
    "- Province - Which province the produce was grown\n",
    "- Container - The code for the type of container\n",
    "- Size_Grade - The class of the produce according to their quality\n",
    "- Weight_Kg - The total weight in kilograms\n",
    "- Commodities - The type of produce, e.g. Golden Delicious Apples\n",
    "- Date - The date of the sale\n",
    "- Sales_Total - The total sales price\n",
    "- Total_Qty_Sold - The number of produce sold\n",
    "- Total_Kg_Sold - The total weight of the produce sold in kilograms\n",
    "- Stock_On_Hand - The amount of produce the store has"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Table of Contents</h1>\n",
    "<ul>\n",
    "    <li><a href=\"#eda\">Exploratiory Data Analysis</a></li>\n",
    "    <li><a href=\"#modeldevelopment\">Model Development</a></li>\n",
    "    <li><a href=\"#modelevaluation\">Model Selection</a></li>\n",
    "    <li><a href=\"#modelselection\">Model Selection</a></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression models\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Model slection\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Cross-validation \n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "# Visualisations\n",
    "#from sklearn.tree import plot_tree\n",
    "##import missingno\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Other\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import math\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"edsa\"> 1) Exploratory Data Analysis   </h1>\n",
    "<p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Analysis\n",
    "\n",
    "#### We want to investigate what data we obtained from the Durban Fresh Produce Market and how much sales they made during the year 2020.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1) Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import the data\n",
    "train = pd.read_csv('df-train_set.csv')\n",
    "test = pd.read_csv('df-test_set.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 ) Dataframe visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Province</th>\n",
       "      <th>Container</th>\n",
       "      <th>Size_Grade</th>\n",
       "      <th>Weight_Kg</th>\n",
       "      <th>Commodities</th>\n",
       "      <th>Date</th>\n",
       "      <th>Low_Price</th>\n",
       "      <th>High_Price</th>\n",
       "      <th>Sales_Total</th>\n",
       "      <th>Total_Qty_Sold</th>\n",
       "      <th>Total_Kg_Sold</th>\n",
       "      <th>Stock_On_Hand</th>\n",
       "      <th>avg_price_per_kg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CAPE</td>\n",
       "      <td>EC120</td>\n",
       "      <td>1L</td>\n",
       "      <td>12.0</td>\n",
       "      <td>APPLE GRANNY SMITH</td>\n",
       "      <td>2020-03-10</td>\n",
       "      <td>108.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>3236.0</td>\n",
       "      <td>29</td>\n",
       "      <td>348.0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CAPE</td>\n",
       "      <td>M4183</td>\n",
       "      <td>1L</td>\n",
       "      <td>18.3</td>\n",
       "      <td>APPLE GOLDEN DELICIOUS</td>\n",
       "      <td>2020-09-09</td>\n",
       "      <td>150.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>51710.0</td>\n",
       "      <td>332</td>\n",
       "      <td>6075.6</td>\n",
       "      <td>822</td>\n",
       "      <td>8.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GAUTENG</td>\n",
       "      <td>AT200</td>\n",
       "      <td>1L</td>\n",
       "      <td>20.0</td>\n",
       "      <td>AVOCADO PINKERTON</td>\n",
       "      <td>2020-05-05</td>\n",
       "      <td>70.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>4860.0</td>\n",
       "      <td>66</td>\n",
       "      <td>1320.0</td>\n",
       "      <td>50</td>\n",
       "      <td>3.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRANSVAAL</td>\n",
       "      <td>BJ090</td>\n",
       "      <td>1L</td>\n",
       "      <td>9.0</td>\n",
       "      <td>TOMATOES-LONG LIFE</td>\n",
       "      <td>2020-01-20</td>\n",
       "      <td>60.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>600.0</td>\n",
       "      <td>10</td>\n",
       "      <td>90.0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>WESTERN FREESTATE</td>\n",
       "      <td>PP100</td>\n",
       "      <td>1R</td>\n",
       "      <td>10.0</td>\n",
       "      <td>POTATO SIFRA (WASHED)</td>\n",
       "      <td>2020-07-14</td>\n",
       "      <td>40.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>41530.0</td>\n",
       "      <td>927</td>\n",
       "      <td>9270.0</td>\n",
       "      <td>393</td>\n",
       "      <td>4.48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Province Container Size_Grade  Weight_Kg             Commodities  \\\n",
       "0               CAPE     EC120         1L       12.0      APPLE GRANNY SMITH   \n",
       "1               CAPE     M4183         1L       18.3  APPLE GOLDEN DELICIOUS   \n",
       "2            GAUTENG     AT200         1L       20.0       AVOCADO PINKERTON   \n",
       "3          TRANSVAAL     BJ090         1L        9.0      TOMATOES-LONG LIFE   \n",
       "4  WESTERN FREESTATE     PP100         1R       10.0   POTATO SIFRA (WASHED)   \n",
       "\n",
       "         Date  Low_Price  High_Price  Sales_Total  Total_Qty_Sold  \\\n",
       "0  2020-03-10      108.0       112.0       3236.0              29   \n",
       "1  2020-09-09      150.0       170.0      51710.0             332   \n",
       "2  2020-05-05       70.0        80.0       4860.0              66   \n",
       "3  2020-01-20       60.0        60.0        600.0              10   \n",
       "4  2020-07-14       40.0        45.0      41530.0             927   \n",
       "\n",
       "   Total_Kg_Sold  Stock_On_Hand  avg_price_per_kg  \n",
       "0          348.0              0              9.30  \n",
       "1         6075.6            822              8.51  \n",
       "2         1320.0             50              3.68  \n",
       "3           90.0              0              6.67  \n",
       "4         9270.0            393              4.48  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.1 ) DataFrame Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table Columns : ['province', 'container', 'size_grade', 'weight_kg', 'commodities', 'date', 'low_price', 'high_price', 'sales_total', 'total_qty_sold', 'total_kg_sold', 'stock_on_hand', 'avg_price_per_kg']\n"
     ]
    }
   ],
   "source": [
    "print('Table Columns :' ,list(train.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.columns = map(str.lower, train.columns) #for accessability and consistency in analysing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 ) Checking dataframe data type and if Null values exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 64376 entries, 0 to 64375\n",
      "Data columns (total 13 columns):\n",
      "province            64376 non-null object\n",
      "container           64376 non-null object\n",
      "size_grade          64376 non-null object\n",
      "weight_kg           64376 non-null float64\n",
      "commodities         64376 non-null object\n",
      "date                64376 non-null object\n",
      "low_price           64376 non-null float64\n",
      "high_price          64376 non-null float64\n",
      "sales_total         64376 non-null float64\n",
      "total_qty_sold      64376 non-null int64\n",
      "total_kg_sold       64376 non-null float64\n",
      "stock_on_hand       64376 non-null int64\n",
      "avg_price_per_kg    64376 non-null float64\n",
      "dtypes: float64(6), int64(2), object(5)\n",
      "memory usage: 6.4+ MB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.3 ) Dataframe information content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64376, 13)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data shows that there are 13 feautures which have 64376 observations which were recorded in the year 2020 and Durban Fresh Market. There are 5 ordinal features with 8 quantitative features of which 2 are discrete features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Investigating the suppliers that Produce Market has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['APPLE GRANNY SMITH', 'APPLE GOLDEN DELICIOUS', 'AVOCADO PINKERTON', 'TOMATOES-LONG LIFE', 'POTATO SIFRA (WASHED)', 'NOVAS', 'SATSUMAS', 'GARLIC IMPORTED', 'LEMON', 'CUCUMBERS ENGLISH', 'BANANA', 'TOMATOES OTHER', 'APPLE BRAEBURN', 'GRAPE OTHER WHITE', 'BANANA FINGERS SINGLE', 'ONIONS BROWN', 'POTATO PANAMERA', 'POTATO (WASHED) MONDIAL', 'GRAPE MIDNIGHT BEAUTY', 'PEACH OTHER', 'BUTTERNUTS', 'POTATO (WASHED) UP-TO-DATE', 'PEAR PACKHAMS TRIUMPH', 'APPLE GALA', 'SWEET POTATOES ORANGE', 'SWEET POTATOES RED', 'AUTUMN CRUNCH', 'ONIONS MILD', 'ORANGE NAVELS', 'PEAR FORELLE', 'WATERMELON', 'TOMATOES SALADETTE', 'AVOCADO RYAN', 'NADORCOTT', 'POTATO LANORMA (WASHED)', 'ORANGE OTHER', 'APPLE FUJI', 'GRAPEFRUIT STAR RUBY', 'APPLE ROYAL GALA', 'APPLE OTHER', 'PEACH SUMMER SUN', 'TOMATOES JAM', 'BANANA LOWER GRADE (OTHER)', 'GRAPE CRIMSON SEEDLESS', 'APPLE CRIPPS PINK', 'ORANGE VALENCIAS', 'PEACH SPRINGCREST', 'GARLIC PURPLE', 'ONIONS WHITE', 'NECTARINE OTHER', 'NECTARINE AUGUST RED', 'ONIONS RED', 'APPLE RED DELICIOUS', 'STARLIGHT', 'CLEMENTINES', 'PLUM SONGOLD', 'APPLE CRIPPS RED', 'NECTARINES SPRING BRIGHT', 'APPLE PINK LADY', 'PLUM- AFRICAN ROSE', 'APPLE TOP RED', 'PEACH TRANSVALIA', 'PLUM ANGELINO', 'ONIONS PICKLED MILD', 'GRAPE DAUPHINE', 'POTATO (WASHED) OTHER', 'NECTARINES AUGUST BRIGHT', 'POTATO (WASHED) HERTHA', 'GARLIC LOCAL', 'MANGO HEIDI', 'PLUM RUBY RED', 'ORANGE MIDNIGHTS', 'PEACH KAKAMAS', 'NAARTJIE', 'APPLE STARKING', 'LIMES', 'ORANGE CARA CARA', 'SONET', 'POTATO VALOR (WASHED)', 'PLUM LAETITIA', 'ONIONS PICKLED BROWN', 'MARROWS BABY', 'GREEN JUICE', 'GRAPE HANEPOOT WHITE', 'PLUM-  RUBY STAR', 'MARROWS LIGHT GREEN', 'TOMATOES COCKTAIL', 'PLUM FLAVORKING', 'POTATO PIMPERNEL (WASHED)', 'PLUM LADY RED', 'NECTARINE ALPINE', 'AVOCADO HASS', 'APPLE EARLY RED ONE', 'PEACHES WITZENBURG', 'AVOCADO FUERTE', 'PLUM LARRY ANNE', 'NECTARINES SUMMER FIRE', 'GRAPE WHITE SEEDLESS', 'CASCADE', 'PLUM PIONEER', 'PLUM AFRICAN PRIDE', 'NECTARINE FLAVORTOP', 'PEACH KEIMOES', 'ONIONS OTHER', 'GRAPES - ALPHA RED', 'APPLE AFRICAN RED/CARMINE', 'AVOCADO OTHER', 'PEAR ROSEMARIE', 'PEACH YELLOW CLING OTHER', 'MARROWS', 'PLUM SUNKISS', 'GRAPE THOMSON SEEDLESS', 'GRAPEFRUIT MARSH', 'SWEET POTATOES BONITA', 'MANGO OTHER', 'PLUM - AFRICAN DELIGHT', 'PEAR OTHER', 'CHERRIES OTHER', 'PEACH OOM SAREL', 'PLUM FORTUNE', 'GRAPE RED GLOBE', 'APPLE ROYAL BEAUT', 'PLUM- BLACK DIAMOND', 'PEAR BUERRE SUPERFIN', 'EVANS DELIGHT', 'SUPREME', 'PEACH KEISIE', 'GRAPE- SCARLOTTA', 'PEAR WILLIAM BON CHRETIEN', 'PLUM SUN SUPREME', 'VALLEY GOLD', 'GRAPE SUGRAONE', 'ABC JUICE', 'PEACH BONNIGOLD', 'GRAPE REGAL SEEDLESS', 'NECTARINES FIRE PEARL', 'PEAR BEURRE HARDY', 'POTATO MARKIES (WASHED)', 'GRAPE OTHER BLACK', 'ONIONS PICKLED WHITE', 'NECTARINES CRIMSON BLAZE', 'POTATO LANORMA', 'APRICOT OTHER', 'PLUM SAPPHIRE', 'SWEET POTATOES ORLEANS', 'PEACH FAIRTIME', 'POTATO (WASHED) BP1', 'KANZI', 'SABLE SEEDLESS', 'APPLE PANORAMA GOLDEN', 'MINNEOLA', 'PEACH CATHERINA', 'GRAPE FLAME SEEDLESS', 'GRAPES - SWEET CELEBRATION', 'PEAR ABATE FETEL', 'JIM DANDY', 'PLUM SOUVENIR', 'GRAPE PRIME SEEDLESS', 'GRAPE RED SULTANA', 'TANGELOS', 'PEACH SAFARI', 'PEAR CONFERENCE', 'SUGRA 13', 'APRICOT IMPERIAL', 'MANGO TOMMY ATKINS', 'APPLE SUNDOWNER', 'MANGO KENT', 'GRAPE DAN BEN HANNAH', 'AUTUMN ROYAL', 'PEAR VERMONT BEAUTY', 'SHARON FRUIT', 'ORANGE DELTA SEEDLESS', 'NECTARINES RED PEARL', 'POTATO MONDIAL', 'TAWNY SEEDLESS', 'DRAGON FRUIT', 'MANGO KEITT', 'PEACH SANDVLIET', 'SUGRA 16', 'PEACHES FANTASY', 'AVOCADO REED', 'AVOCADO OSMERIAS', 'PEACH ORION', 'GRAPE VICTORIA', 'NECTARINES ROSE BRIGHT', 'PLUM SOUTHERN BELLE', 'SWEET POTATOES WHITE', 'PEAR FLAMINGO', 'PLUM RED ACE', 'MANGO ROSA', 'APPLE ROME BEAUTY', 'PLUM RED GOLD', 'PEACH SUMMER GIANT', 'POTATO OTHER', 'AFRICAN SUNSET', 'POTATO UP-TO-DATE', 'PEACH EARLY BELLE', 'APPLE SPLENDOUR', 'MOONBALLS', 'POTATO EOS (WASHED)', 'GRAPE SUNRED SEEDLESS LHARVEST', 'GRAPE RED EMPEROR', 'PEAR BEURRY BOSCH', 'SWEET POTATOES PURPLE', 'POTATO MARKIES', 'PLUM-  FLAVOROSA', 'POTATO VALOR', 'PEACH WESTERN SUN', 'PRUNES', 'NECTARINE FANTASIA', 'AVOCADO ETTINGER', 'MANGO SENSATION', 'NECTARINE FLAMEKIST', 'GRAPE BONITA', 'AVOCADO MAHA', 'GRAPE BARLINKA', 'DECEMBER PRINCESS', 'POTATO (WASHED) VANDERPLANK', 'GRAPE BLACK EMPEROR', 'MANGO ZILL', 'POTATO KIKKO (WASHED)', 'POTATO LADY ROSETTA', 'GRAPE RED SEEDED GRAPE', 'NECTARINES ARTIC MIST', 'GRAPE ALPHONSE LAVALLEE', 'POTATO MNANDI', 'WASHED POTATOES', 'APPLE CAMEO', 'PLUM- SUPLUMTWENTY EIGHT', 'POTATO (WASHED) MNANDI', 'PEACH NOVADONNA', 'AVOCADO EDRANOL', 'NECTARINES NECTAR GALA', 'PEACH ALBERTA', 'PEAR CLAPPS FAVOURITE', 'GRAPE DATAL', 'GUAVAS', 'NECTARINES ARTIC SPRING', 'SEPTEMBER YUMMY', 'ONIONS PICKLED RED', 'PEACH EARLIGRANDE', 'PEACH WESTERN CLING', 'MELODY', 'MANGO PEACH', 'APPLE JONAGOLD', 'APPLE STAR KRIMSON', 'GRAPE LA ROCHELLE', 'PEACH EXCELLENCE', 'GRAPE BLACK GEM', 'GRAPE SUNDANCE', 'PEACHES BRITTNEY LANE', 'CHERRIES BING', 'PEAR BON ROUGE', 'COTTON CANDY', 'PLUM RED BEAUT', 'SWEET POTATOES BLESBOK', 'PEACHES PERIGREEN', 'AVOCADO SIMMONS', 'POTATO SIFRA', 'GRAPE SALBA', 'PURPLE MAJESTY', 'PLUM LARODA', 'GRAPE SUPER THOMSON', 'TAMBORS', 'PLUM PRESIDENT', 'PLUM RUBY NEL', 'GRAPE MUSCAT DELIGHT', 'PLUM SANTA ROSA', 'PLUM GOLDEN KING', 'PLUM-  SUPLUM SIX']\n"
     ]
    }
   ],
   "source": [
    "commodities=list(train.commodities.unique())\n",
    "print(commodities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of supplier that Durban Fresh Produce Market is 270\n"
     ]
    }
   ],
   "source": [
    "# The number of suppliers \n",
    "print(f'The number of supplier that Durban Fresh Produce Market is {len(commodities)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Durban Fresh Produce Market has 270 suppliers as indicated by the commodities column. We are only required to predict price for supplier which is Golden Delicious Apple which under Commodities is written as **APPLE GOLDEN DELICIOUS**. We are only going to focus the supplier mentioned from the above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Filtering data to only Golden Delicious Apples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>province</th>\n",
       "      <th>container</th>\n",
       "      <th>size_grade</th>\n",
       "      <th>weight_kg</th>\n",
       "      <th>commodities</th>\n",
       "      <th>date</th>\n",
       "      <th>low_price</th>\n",
       "      <th>high_price</th>\n",
       "      <th>sales_total</th>\n",
       "      <th>total_qty_sold</th>\n",
       "      <th>total_kg_sold</th>\n",
       "      <th>stock_on_hand</th>\n",
       "      <th>avg_price_per_kg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CAPE</td>\n",
       "      <td>M4183</td>\n",
       "      <td>1L</td>\n",
       "      <td>18.3</td>\n",
       "      <td>APPLE GOLDEN DELICIOUS</td>\n",
       "      <td>2020-09-09</td>\n",
       "      <td>150.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>51710.0</td>\n",
       "      <td>332</td>\n",
       "      <td>6075.6</td>\n",
       "      <td>822</td>\n",
       "      <td>8.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>CAPE</td>\n",
       "      <td>JG110</td>\n",
       "      <td>2M</td>\n",
       "      <td>11.0</td>\n",
       "      <td>APPLE GOLDEN DELICIOUS</td>\n",
       "      <td>2020-04-14</td>\n",
       "      <td>50.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>16000.0</td>\n",
       "      <td>320</td>\n",
       "      <td>3520.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>W.CAPE-BERGRIVER ETC</td>\n",
       "      <td>JE090</td>\n",
       "      <td>2S</td>\n",
       "      <td>9.0</td>\n",
       "      <td>APPLE GOLDEN DELICIOUS</td>\n",
       "      <td>2020-04-16</td>\n",
       "      <td>55.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>990.0</td>\n",
       "      <td>18</td>\n",
       "      <td>162.0</td>\n",
       "      <td>1506</td>\n",
       "      <td>6.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>CAPE</td>\n",
       "      <td>M4183</td>\n",
       "      <td>1S</td>\n",
       "      <td>18.3</td>\n",
       "      <td>APPLE GOLDEN DELICIOUS</td>\n",
       "      <td>2020-05-04</td>\n",
       "      <td>80.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>32020.0</td>\n",
       "      <td>388</td>\n",
       "      <td>7100.4</td>\n",
       "      <td>443</td>\n",
       "      <td>4.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>EASTERN CAPE</td>\n",
       "      <td>IA400</td>\n",
       "      <td>1S</td>\n",
       "      <td>400.0</td>\n",
       "      <td>APPLE GOLDEN DELICIOUS</td>\n",
       "      <td>2020-09-28</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>1</td>\n",
       "      <td>400.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4.50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                province container size_grade  weight_kg  \\\n",
       "1                   CAPE     M4183         1L       18.3   \n",
       "7                   CAPE     JG110         2M       11.0   \n",
       "24  W.CAPE-BERGRIVER ETC     JE090         2S        9.0   \n",
       "40                  CAPE     M4183         1S       18.3   \n",
       "69          EASTERN CAPE     IA400         1S      400.0   \n",
       "\n",
       "               commodities        date  low_price  high_price  sales_total  \\\n",
       "1   APPLE GOLDEN DELICIOUS  2020-09-09      150.0       170.0      51710.0   \n",
       "7   APPLE GOLDEN DELICIOUS  2020-04-14       50.0        50.0      16000.0   \n",
       "24  APPLE GOLDEN DELICIOUS  2020-04-16       55.0        55.0        990.0   \n",
       "40  APPLE GOLDEN DELICIOUS  2020-05-04       80.0       120.0      32020.0   \n",
       "69  APPLE GOLDEN DELICIOUS  2020-09-28     1800.0      1800.0       1800.0   \n",
       "\n",
       "    total_qty_sold  total_kg_sold  stock_on_hand  avg_price_per_kg  \n",
       "1              332         6075.6            822              8.51  \n",
       "7              320         3520.0              0              4.55  \n",
       "24              18          162.0           1506              6.11  \n",
       "40             388         7100.4            443              4.51  \n",
       "69               1          400.0              2              4.50  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#filtering the data to what we need to look at: Golden Delicious Apples\n",
    "train = train[train['commodities'] == 'APPLE GOLDEN DELICIOUS']\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['province', 'container', 'size_grade', 'weight_kg', 'commodities',\n",
       "       'date', 'low_price', 'high_price', 'sales_total', 'total_qty_sold',\n",
       "       'total_kg_sold', 'stock_on_hand', 'avg_price_per_kg'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1952 entries, 1 to 64310\n",
      "Data columns (total 13 columns):\n",
      "province            1952 non-null object\n",
      "container           1952 non-null object\n",
      "size_grade          1952 non-null object\n",
      "weight_kg           1952 non-null float64\n",
      "commodities         1952 non-null object\n",
      "date                1952 non-null object\n",
      "low_price           1952 non-null float64\n",
      "high_price          1952 non-null float64\n",
      "sales_total         1952 non-null float64\n",
      "total_qty_sold      1952 non-null int64\n",
      "total_kg_sold       1952 non-null float64\n",
      "stock_on_hand       1952 non-null int64\n",
      "avg_price_per_kg    1952 non-null float64\n",
      "dtypes: float64(6), int64(2), object(5)\n",
      "memory usage: 213.5+ KB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1952, 13)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data shows that there are 13 feautures which have 1952 observations which were recorded in the year 2020 and Durban Fresh Market. There are 3 ordinal features namely Province,container and size_grade. There is also 8 numerical features which are continuous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 ) Rechecking for Null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "province            0\n",
       "container           0\n",
       "size_grade          0\n",
       "weight_kg           0\n",
       "commodities         0\n",
       "date                0\n",
       "low_price           0\n",
       "high_price          0\n",
       "sales_total         0\n",
       "total_qty_sold      0\n",
       "total_kg_sold       0\n",
       "stock_on_hand       0\n",
       "avg_price_per_kg    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data has no null/ missing data thus there is no need for data cleaning at the moment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 ) Setting the columns to appropriate data types \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the features such as Date are in an incorrect format or their datatype will make analysis and even prediction more difficult to do. Thus, setting the datatypes is a critical step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Set datetime data \n",
    "train[\"date\"] = pd.to_datetime(train[\"date\"],format=\"%Y.%m.%d\")\n",
    "\n",
    "#Set categorical data\n",
    "train['province'] = train['province'].astype('category')\n",
    "train['container'] = train['container'].astype('category')\n",
    "train['size_grade'] = train['size_grade'].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The improved dataset with appropriate data types as per feature characteristic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1952 entries, 1 to 64310\n",
      "Data columns (total 13 columns):\n",
      "province            1952 non-null category\n",
      "container           1952 non-null category\n",
      "size_grade          1952 non-null category\n",
      "weight_kg           1952 non-null float64\n",
      "commodities         1952 non-null object\n",
      "date                1952 non-null datetime64[ns]\n",
      "low_price           1952 non-null float64\n",
      "high_price          1952 non-null float64\n",
      "sales_total         1952 non-null float64\n",
      "total_qty_sold      1952 non-null int64\n",
      "total_kg_sold       1952 non-null float64\n",
      "stock_on_hand       1952 non-null int64\n",
      "avg_price_per_kg    1952 non-null float64\n",
      "dtypes: category(3), datetime64[ns](1), float64(6), int64(2), object(1)\n",
      "memory usage: 174.6+ KB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 ) Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  3.1) Univariate Analysis: Non-Graphical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For the numerical data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weight_kg</th>\n",
       "      <th>low_price</th>\n",
       "      <th>high_price</th>\n",
       "      <th>sales_total</th>\n",
       "      <th>total_qty_sold</th>\n",
       "      <th>total_kg_sold</th>\n",
       "      <th>stock_on_hand</th>\n",
       "      <th>avg_price_per_kg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1952.000000</td>\n",
       "      <td>1952.000000</td>\n",
       "      <td>1952.000000</td>\n",
       "      <td>1952.000000</td>\n",
       "      <td>1952.000000</td>\n",
       "      <td>1952.000000</td>\n",
       "      <td>1952.000000</td>\n",
       "      <td>1952.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>40.460912</td>\n",
       "      <td>174.307377</td>\n",
       "      <td>215.648053</td>\n",
       "      <td>20053.533811</td>\n",
       "      <td>174.510758</td>\n",
       "      <td>2960.176332</td>\n",
       "      <td>408.393955</td>\n",
       "      <td>6.778893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>99.655169</td>\n",
       "      <td>373.553578</td>\n",
       "      <td>433.546159</td>\n",
       "      <td>39005.069445</td>\n",
       "      <td>308.810797</td>\n",
       "      <td>6097.416527</td>\n",
       "      <td>724.450582</td>\n",
       "      <td>2.248744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>9.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>1325.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>219.600000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>5.460000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>12.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>108.000000</td>\n",
       "      <td>5495.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>853.500000</td>\n",
       "      <td>126.500000</td>\n",
       "      <td>6.670000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>18.300000</td>\n",
       "      <td>127.250000</td>\n",
       "      <td>160.000000</td>\n",
       "      <td>21082.500000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>3093.525000</td>\n",
       "      <td>468.000000</td>\n",
       "      <td>8.280000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>400.000000</td>\n",
       "      <td>2300.000000</td>\n",
       "      <td>3300.000000</td>\n",
       "      <td>369464.000000</td>\n",
       "      <td>4237.000000</td>\n",
       "      <td>74000.000000</td>\n",
       "      <td>6400.000000</td>\n",
       "      <td>21.240000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         weight_kg    low_price   high_price    sales_total  total_qty_sold  \\\n",
       "count  1952.000000  1952.000000  1952.000000    1952.000000     1952.000000   \n",
       "mean     40.460912   174.307377   215.648053   20053.533811      174.510758   \n",
       "std      99.655169   373.553578   433.546159   39005.069445      308.810797   \n",
       "min       3.000000     2.000000     5.000000       5.000000        1.000000   \n",
       "25%       9.000000    50.000000    60.000000    1325.000000       12.000000   \n",
       "50%      12.000000    80.000000   108.000000    5495.000000       64.000000   \n",
       "75%      18.300000   127.250000   160.000000   21082.500000      200.000000   \n",
       "max     400.000000  2300.000000  3300.000000  369464.000000     4237.000000   \n",
       "\n",
       "       total_kg_sold  stock_on_hand  avg_price_per_kg  \n",
       "count    1952.000000    1952.000000       1952.000000  \n",
       "mean     2960.176332     408.393955          6.778893  \n",
       "std      6097.416527     724.450582          2.248744  \n",
       "min         3.000000       0.000000          0.250000  \n",
       "25%       219.600000       9.000000          5.460000  \n",
       "50%       853.500000     126.500000          6.670000  \n",
       "75%      3093.525000     468.000000          8.280000  \n",
       "max     74000.000000    6400.000000         21.240000  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Descriptive Statistics\n",
    "train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "From observing the numerical data there are many errors in the data :\n",
    "* The mean of the features is quite distinct from the median which indicates there are outliers present\n",
    "\n",
    "To check the distribution of the data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "weight_kg           3.328254\n",
       "low_price           4.222672\n",
       "high_price          3.828819\n",
       "sales_total         4.250317\n",
       "total_qty_sold      4.445169\n",
       "total_kg_sold       5.529586\n",
       "stock_on_hand       3.391815\n",
       "avg_price_per_kg    0.082452\n",
       "dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking distrubution of the data\n",
    "train.skew()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The skewness of the data tells us how each numerical is distributed and how symmetrical it is. It is observed that:\n",
    " * All of our features except for the average price per kg are extremely positive skewed indicate by the values being \n",
    "   greater than 1.\n",
    " * The Average price per kg feature which is our target variable is fairly symetrical the skew value of 0.082452 is between -0.5 and 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "weight_kg            9.109698\n",
       "low_price           17.073369\n",
       "high_price          13.630768\n",
       "sales_total         24.200524\n",
       "total_qty_sold      32.338123\n",
       "total_kg_sold       45.153801\n",
       "stock_on_hand       14.671224\n",
       "avg_price_per_kg     1.042921\n",
       "dtype: float64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking for outliers in the data\n",
    "train.kurt()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above we can see that there is a high number of outliers present in our observations as we have observed high kurtosis(**>3**) for all the features except for the target feature,avg_price_per_kg with kurtosis value 1.042921. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Categorical Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>province</th>\n",
       "      <th>container</th>\n",
       "      <th>size_grade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1952</td>\n",
       "      <td>1952</td>\n",
       "      <td>1952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>CAPE</td>\n",
       "      <td>M4183</td>\n",
       "      <td>1S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1045</td>\n",
       "      <td>588</td>\n",
       "      <td>532</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       province container size_grade\n",
       "count      1952      1952       1952\n",
       "unique        7        11         10\n",
       "top        CAPE     M4183         1S\n",
       "freq       1045       588        532"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.describe(include=['category'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The categorical data description shows us the mode which is indicated by top and the frequncy of data. The data show that most apples came from Cape province which had the size_grad of 1S and most of the stored in the container: M4183."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate Analysis: Graphical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'seaborn' has no attribute 'set_theme'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-b66b55582e40>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_theme\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstyle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"whitegrid\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: module 'seaborn' has no attribute 'set_theme'"
     ]
    }
   ],
   "source": [
    "sns.set_theme(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numerical Features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features =list(train.select_dtypes(include=['float64']).columns)\n",
    "numerical_features = numerical_features +list(train.select_dtypes(include=['int64']).columns)\n",
    "\n",
    "train[numerical_features].hist()\n",
    "plt.gcf().set_size_inches(10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax =plt.subplots(1,2)\n",
    "sns.violinplot(x = 'low_price', data = train,ax=ax[0])\n",
    "sns.violinplot(x = 'high_price', data = train,ax=ax[1])\n",
    "#sns.violinplot(x = 'avg_price_per_kg',data = train, ax=ax[2])\n",
    "plt.gcf().set_size_inches(15, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.boxplot(x = 'avg_price_per_kg', data = train)\n",
    "plt.gcf().set_size_inches(15, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical Features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = sns.countplot(x = 'province',data = train,order = train['province'].value_counts().index)\n",
    "\n",
    "graph.set_xticklabels(graph.get_xticklabels(),rotation=0)\n",
    "for p in graph.patches:\n",
    "    height = p.get_height()\n",
    "    graph.text(p.get_x()+p.get_width()/2., height + 0.1,height ,ha=\"center\")\n",
    "plt.gcf().set_size_inches(15, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = sns.countplot(x = 'container', data = train,order = train['container'].value_counts().index)\n",
    "\n",
    "graph.set_xticklabels(graph.get_xticklabels(),rotation=0)\n",
    "for p in graph.patches:\n",
    "    height = p.get_height()\n",
    "    graph.text(p.get_x()+p.get_width()/2., height + 0.1,height ,ha=\"center\")\n",
    "    \n",
    "plt.gcf().set_size_inches(15, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = sns.countplot(x = 'size_grade', data = train,order = train['size_grade'].value_counts().index)\n",
    "\n",
    "graph.set_xticklabels(graph.get_xticklabels(),rotation=0)\n",
    "for p in graph.patches:\n",
    "    height = p.get_height()\n",
    "    graph.text(p.get_x()+p.get_width()/2., height + 0.1,height ,ha=\"center\")\n",
    "    \n",
    "plt.gcf().set_size_inches(15, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with outliers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shape_before = train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_numerical = train.loc[:, 'weight_kg':'avg_price_per_kg'].drop(columns=['commodities','date'])\n",
    "train = train[(np.abs(stats.zscore(train_numerical)) < 3).all(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of ouliers removed:\", shape_before - train.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After outliers:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate Analysis: Non-Graphical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.skew()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.kurt()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Categorical Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.describe(include=['category'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate Analysis: Graphical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features =list(train.select_dtypes(include=['float64']).columns)\n",
    "numerical_features = numerical_features +list(train.select_dtypes(include=['int64']).columns)\n",
    "\n",
    "train[numerical_features].hist()\n",
    "plt.gcf().set_size_inches(20, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[numerical_features].plot(kind='density', subplots=True, layout=(3, 3), sharex=False)\n",
    "\n",
    "plt.gcf().set_size_inches(20, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax =plt.subplots(1,2)\n",
    "sns.violinplot(x = 'low_price', data = train,ax=ax[0])\n",
    "sns.violinplot(x = 'high_price', data = train,ax=ax[1])\n",
    "\n",
    "plt.gcf().set_size_inches(15, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = sns.countplot(x = 'province',data = train,order = train['province'].value_counts().index)\n",
    "\n",
    "graph.set_xticklabels(graph.get_xticklabels(),rotation=0)\n",
    "for p in graph.patches:\n",
    "    height = p.get_height()\n",
    "    graph.text(p.get_x()+p.get_width()/2., height + 0.1,height ,ha=\"center\")\n",
    "plt.gcf().set_size_inches(15, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = sns.countplot(x = 'container', data = train,order = train['container'].value_counts().index)\n",
    "\n",
    "graph.set_xticklabels(graph.get_xticklabels(),rotation=0)\n",
    "for p in graph.patches:\n",
    "    height = p.get_height()\n",
    "    graph.text(p.get_x()+p.get_width()/2., height + 0.1,height ,ha=\"center\")\n",
    "    \n",
    "plt.gcf().set_size_inches(15, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = sns.countplot(x = 'size_grade', data = train,order = train['size_grade'].value_counts().index)\n",
    "\n",
    "graph.set_xticklabels(graph.get_xticklabels(),rotation=0)\n",
    "for p in graph.patches:\n",
    "    height = p.get_height()\n",
    "    graph.text(p.get_x()+p.get_width()/2., height + 0.1,height ,ha=\"center\")\n",
    "    \n",
    "plt.gcf().set_size_inches(15, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Feature Analysis : Average price per kg\n",
    "\n",
    "The target feature is the average price per kg of the Golden Delicous Apples\n",
    " * non graphical approach\n",
    " * graphical analysis\n",
    " \n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.avg_price_per_kg.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.avg_price_per_kg.mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.avg_price_per_kg.skew()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.avg_price_per_kg.kurt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x = 'avg_price_per_kg', data = train)\n",
    "plt.gcf().set_size_inches(15, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=train, x=\"avg_price_per_kg\", stat=\"probability\", discrete=True, kde = True)\n",
    "\n",
    "plt.gcf().set_size_inches(15, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariate Analysis: Non-Graphical "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariate Analysis: Graphical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap = sns.heatmap(train.corr(), vmin=-1, vmax=1, annot=True)\n",
    "heatmap.set_title('Correlation Heatmap', fontdict={'fontsize':12}, pad=12);\n",
    "plt.gcf().set_size_inches(15, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(train, col=\"province\",hue = 'size_grade')\n",
    "g.map(sns.scatterplot, \"weight_kg\", \"avg_price_per_kg\", alpha=1)\n",
    "plt.gcf().set_size_inches(20, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(train, col=\"province\")\n",
    "g.map(sns.scatterplot, \"low_price\", \"avg_price_per_kg\", alpha=1)\n",
    "plt.gcf().set_size_inches(20, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(train, col=\"province\")\n",
    "g.map(sns.scatterplot, \"high_price\", \"avg_price_per_kg\", alpha=1)\n",
    "plt.gcf().set_size_inches(20, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(train, col=\"province\")\n",
    "g.map(sns.scatterplot, \"sales_total\", \"avg_price_per_kg\", alpha=1)\n",
    "plt.gcf().set_size_inches(20, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x=\"date\", y=\"avg_price_per_kg\",\n",
    "             data=train)\n",
    "plt.gcf().set_size_inches(15, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x=\"date\", y=\"weight_kg\",\n",
    "             data=train)\n",
    "plt.gcf().set_size_inches(15, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptive Statistical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.describe(include=['category'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Value counts is a good way of understanding how many units we have for each variable. **NB:** value_counts() works only on a pandas series, not a pandas DataFrame.\n",
    "\n",
    "However, we can convert the value count series into a DataFrame using to_frame()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "province_counts = train['province'].value_counts().to_frame()\n",
    "province_counts.rename(columns={'province': 'value_counts'}, inplace=True)\n",
    "province_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#renaming the index to province\n",
    "province_counts.index.name = 'province'\n",
    "province_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_counts = train['size_grade'].value_counts().to_frame()\n",
    "size_counts.rename(columns={'size_grade': 'value_counts'}, inplace=True)\n",
    "size_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "container_counts = train['container'].value_counts().to_frame()\n",
    "container_counts.rename(columns={'container': 'value_counts'}, inplace=True)\n",
    "container_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from all these 3 value_count dataframes of 'province', 'size_grade' and 'container', the results are not skewed and hence the conclusion is that the 3 categorical features are reliable predictor variables of price. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation and Causation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The p value is the evidence against a null hypothesis. The smaller the p-value, the stronger the evidence that you should reject the null hypothesis.\n",
    "\n",
    "By convention, when the\n",
    "\n",
    "- p-value is  <  0.001: we say there is strong evidence that the correlation is significant.\n",
    "- the p-value is  <  0.05: there is moderate evidence that the correlation is significant.\n",
    "- the p-value is  <  0.1: there is weak evidence that the correlation is significant.\n",
    "- the p-value is  >  0.1: there is no evidence that the correlation is significant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Weight vs. Average Price per Kg**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pearson_coef, p_value = stats.pearsonr(train['weight_kg'], train['avg_price_per_kg'])\n",
    "print(\"The Pearson Correlation Coefficient is\", pearson_coef, \" with a P-value of P =\", p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Conclusion_** **:** Since the P-value is far less than 0.001, we can conclude that the p value between weight and average price per kg is statistically signifigcant although the linear relationship isn't extremely strong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Low Price vs. Average Price per Kg**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pearson_coef, p_value = stats.pearsonr(train['low_price'], train['avg_price_per_kg'])\n",
    "print(\"The Pearson Correlation Coefficient is\", pearson_coef, \" with a P-value of P =\", p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Conclusion_** **:** Since the P-value is far less than 0.001, we can conclude that the p value between low price and average price per kg is statistically signifigcant and the linear relationship is quite strong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**High Price vs. Average Price per Kg**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pearson_coef, p_value = stats.pearsonr(train['high_price'], train['avg_price_per_kg'])\n",
    "print(\"The Pearson Correlation Coefficient is\", pearson_coef, \" with a P-value of P =\", p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Conclusion_** **:** Since the P-value is far less than 0.001, we can conclude that the p value between weight and average price per kg is statistically signifigcant and the linear relationship is strong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANOVA "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis of variance is a collection of statistical models and their associated estimation procedures used to analyze the differences among means of two or more groups. The ANOVA test from scipy.stats returns two results:\n",
    "\n",
    "<p><b>F-test score</b>: ANOVA assumes the means of all groups are the same, calculates how much the actual means deviate from the assumption, and reports it as the F-test score. A larger score means there is a larger difference between the means.</p>\n",
    "\n",
    "<p><b>P-value</b>:  P-value tells how statistically significant is our calculated score value.</p>\n",
    "\n",
    "<p>If our price variable is strongly correlated with the variable we are analyzing, expect ANOVA to return a sizeable F-test score and a small p-value.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Provinces**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since ANOVA analyzes the difference between different groups of the same variable, the groupby function will come in handy. Because the ANOVA algorithm averages the data automatically, we do not need to take the average before hand.\n",
    "\n",
    "Let's see if different 'provinces' impact 'avg_price_per_kg.' We group the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_provinces=train[['province', 'avg_price_per_kg']].groupby(['province'])\n",
    "grouped_provinces.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we use the function 'f_oneway' in the module 'stats' to obtain the **F-test score** and **P-value**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANOVA\n",
    "f_val, p_val = stats.f_oneway(grouped_provinces.get_group('CAPE')['avg_price_per_kg'], grouped_provinces.get_group('W.CAPE-BERGRIVER ETC')['avg_price_per_kg'], \n",
    "                              grouped_provinces.get_group('WEST COAST')['avg_price_per_kg'], grouped_provinces.get_group('EASTERN CAPE')['avg_price_per_kg'],\n",
    "                             grouped_provinces.get_group('NATAL')['avg_price_per_kg'], grouped_provinces.get_group('TRANSVAAL')['avg_price_per_kg'],\n",
    "                             grouped_provinces.get_group('ORANGE FREE STATE')['avg_price_per_kg'])  \n",
    " \n",
    "print( \"ANOVA results: F=\", f_val, \", P =\", p_val)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As is turns out, this is a great result, our F-value of 77.15 indicates that the between-groups variance is over 77 times the size of the within-group variance. The null hypothesis value is that variances are equal, which produces an F-value of 1; The F-test score is showing a strong correlation and the result is almost 0 and we can infer that it's amost certain that the result is statistically significant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Size / Grade**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_sizes=train[['size_grade', 'avg_price_per_kg']].groupby(['size_grade'])\n",
    "grouped_sizes.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANOVA\n",
    "f_val, p_val = stats.f_oneway(grouped_sizes.get_group('1L')['avg_price_per_kg'], grouped_sizes.get_group('2M')['avg_price_per_kg'], \n",
    "                              grouped_sizes.get_group('2S')['avg_price_per_kg'], grouped_sizes.get_group('1S')['avg_price_per_kg'],\n",
    "                             grouped_sizes.get_group('1M')['avg_price_per_kg'], grouped_sizes.get_group('1X')['avg_price_per_kg'],\n",
    "                              grouped_sizes.get_group('2L')['avg_price_per_kg'], grouped_sizes.get_group('2U')['avg_price_per_kg'],\n",
    "                              grouped_sizes.get_group('2X')['avg_price_per_kg'], grouped_sizes.get_group('1U')['avg_price_per_kg'])\n",
    " \n",
    "print( \"ANOVA results: F=\", f_val, \", P =\", p_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The F-test score reveals that there's a strong correlation and this is statistically significant result since the P-value << 0.001. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Container**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_containers=train[['container', 'avg_price_per_kg']].groupby(['container'])\n",
    "grouped_containers.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANOVA\n",
    "f_val, p_val = stats.f_oneway(grouped_containers.get_group('M4183')['avg_price_per_kg'], grouped_containers.get_group('JG110')['avg_price_per_kg'], \n",
    "                              grouped_containers.get_group('JE090')['avg_price_per_kg'], grouped_containers.get_group('EC120')['avg_price_per_kg'],\n",
    "                             grouped_containers.get_group('AC030')['avg_price_per_kg'], grouped_containers.get_group('M6125')['avg_price_per_kg'],\n",
    "                              grouped_containers.get_group('EF120')['avg_price_per_kg'], grouped_containers.get_group('DT063')['avg_price_per_kg'],\n",
    "                              grouped_containers.get_group('M9125')['avg_price_per_kg'], grouped_containers.get_group('EG140')['avg_price_per_kg'])\n",
    " \n",
    "print( \"ANOVA results: F=\", f_val, \", P =\", p_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the F-test score reveals that there's a strong correlation and this is statistically significant result since the P-value << 0.001.\n",
    "\n",
    "**We may now conlude that the useful features we can use for building machine learning models are:**\n",
    "\n",
    "- low price\n",
    "- high price \n",
    "- province \n",
    "- size_grade\n",
    "- container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feeding machine learning models with variables that meaningfully affect our target variable will improve our model's prediction accuracy. \n",
    "\n",
    "However, we have to first encode `'province'`, `'size_grade'` and `'container'` features with **dummy variables**, because regression models understand numbers not words or strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dummy_df = pd.get_dummies(train[[\"province\", \"size_grade\", \"container\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge with main df on key values\n",
    "train = pd.concat([train, dummy_df], axis=1)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can drop non-numerical columns in our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = train.drop(columns=[\"commodities\", \"province\", \"size_grade\", \"container\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can still come up with even more useful features to feed into our machine learning models, via a process known as feature engineering. First we create a baseline model and see how it improves when we include the new features that we create. The usefulness of a feature depends on the type of model that you will use. A feature is only useful to the extent that its relationship with the target is one your model can learn.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Baseline Model\n",
    "baseline= RandomForestRegressor(criterion=\"mse\", random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross-Validation Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_cross_neg = cross_val_score(baseline, train[['low_price', 'high_price']], \n",
    "                                train['avg_price_per_kg'], cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "rmse_baseline = - 1 * rmse_cross_neg \n",
    "rmse_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The mean of the cross-validation folds is \", rmse_baseline.mean(), \"and the standard deviation is \", rmse_baseline.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the supply and demand economic model, we may expect that if the demand of golden delicious apples is low and the supply is high, then average price of the apples will be low and viceversa. What we need is a way to quantify the demand and supply in terms of the given features.\n",
    "\n",
    "We expect that the *more* the quantity that's sold, the _*higher* the supply_ and the _*lower* the demand_. The *less* the quatity that's sold, the _*higher* the demand_ and the _*lower* the supply_.\n",
    "\n",
    "One way companies determine how to price their goods is to look at the demand curve and calculate the potential revenue and profit at various prices and quantities:\n",
    "$$\n",
    "(Revenue) = (Quantity Sold) * (Price)\n",
    "$$\n",
    "                                              \n",
    "So it seems plausible that a ratio of our `'sales_total'` and `'total_qty_sold'` might give us and idea about the price of apples. We may also try to include the `'weight_kg'` feature, since our target variable has units of kg as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['price_kg'] = train['sales_total'] * train['weight_kg'] / train['total_qty_sold']  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since most of the values in our data set are from the year 2020, we may therefore try to seek a pattern of the month and the average price per kg. Intuitively, one might expect less apples purchased during strict lockdown levels implemented by the South African government during the pandemic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['month'] = train['date'].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = RandomForestRegressor(criterion=\"mse\", random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_cross_neg = cross_val_score(model, train[[ 'low_price', 'high_price', 'month', 'price_kg']], \n",
    "                                train['avg_price_per_kg'], cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "rmse_model = - 1 * rmse_cross_neg\n",
    "rmse_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The mean of the cross-validation folds is \", rmse_model.mean(), \"and the standard deviation is \", rmse_model.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's been a huge decrease in rmse, this is evidence enough that the `month` and `price_kg` features have exposed some new information and patterns that were not caught by the model before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see what happens when we have dummy variables that we created with the get_dummies method to our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ind_var_df = train.drop(columns=[\"date\", \"avg_price_per_kg\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_cross_neg = cross_val_score(model, ind_var_df, \n",
    "                                train['avg_price_per_kg'], cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "rmse_model = - 1 * rmse_cross_neg\n",
    "rmse_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The mean of the cross-validation folds is \", rmse_model.mean(), \"and the standard deviation is \", rmse_model.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the rmse error is reduced when we include our dummy variables, this is also evidence that letting the model know the province, container and size of apples lets it make better prediction, this was expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"modeldevelopment\">Model Development  </h1>\n",
    "<p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will develop several models that will predict the average price per kg of Golden Delicious Apples using the variables or features that will meaningfully impact our model.\n",
    "\n",
    "In Data Analytics, we often use **Model Development** to help us predict future observations from the data we have.\n",
    "\n",
    "A Model will help us understand the exact relationship between different variables and how these variables are used to predict the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to rescale our data, since because scaling data is a crucial first step to regularisation as the penalty on particular coefficients in regularisation techniques namely ridge regression and lasso regression, depends largely on the scale associated with the variables. There are two common methods to scale data (MinMaxScaler and StandardScaler). We will use standard scaler to scale our data, because it is more robust to ouliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_data = train['avg_price_per_kg']\n",
    "x_data = train.drop(columns= ['date','avg_price_per_kg'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_scaled = scaler.fit_transform(x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_standardise = pd.DataFrame(X_scaled,columns=x_data.columns)\n",
    "X_standardise.describe().loc['std']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the variance of our features is now equal to one after standardization. Which is expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we randomly split our data into training and testing data using the function train_test_split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X_standardise, y_data, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"number of test samples :\", x_test.shape[0])\n",
    "print(\"number of training samples:\",x_train.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 1: Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Simple Linear Regression is a method to help us understand the relationship between two variables:</p>\n",
    "<ul>\n",
    "    <li>The predictor/independent variable (X)</li>\n",
    "    <li>The response/dependent variable (that we want to predict)(Y)</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lre = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lre.fit(x_train[['low_price']], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R^2 on test data \n",
    "lre.score(x_test[['low_price']], y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R^2 on train data \n",
    "lre.score(x_train[['low_price']], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction using train data:\n",
    "y_hat_train_1 = lre.predict(x_train[['low_price']])\n",
    "y_hat_train_1[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction using test data:\n",
    "y_hat_test_1 = lre.predict(x_test[['low_price']])\n",
    "y_hat_test_1[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RSME on training data \n",
    "train_rmse = math.sqrt(mean_squared_error(y_train, y_hat_train_1))\n",
    "train_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RSME on test data\n",
    "test_rsme = math.sqrt(mean_squared_error(y_test, y_hat_test_1))\n",
    "test_rsme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 2: Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to predict the average price per kg of Golden Delicious apples using more than one variable we use Multiple Linear Regression.\n",
    "\n",
    "The following function does the previous steps of model 1. Since the process becomes quit repetitive. We just have to instantiate the model's object and pass it into the function along with the specified x_train and x_test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_and_eval (model, X_train, X_test, Y_train = y_train, Y_test = y_test):\n",
    "    # fit model to data/train model\n",
    "    model.fit(X_train, Y_train) \n",
    "    # prediction on train data\n",
    "    Yhat_train = model.predict(X_train)\n",
    "    # prediction on test data\n",
    "    Yhat_test = model.predict(X_test)\n",
    "    # train rmse \n",
    "    rmse_train = math.sqrt(mean_squared_error(Y_train, Yhat_train))\n",
    "    # test rmse \n",
    "    rmse_test = math.sqrt(mean_squared_error(Y_test, Yhat_test))\n",
    "    estimates_dict = {'R^2 on train data': [model.score(X_train, Y_train)], 'R^2 on test data': [model.score(X_test, Y_test)],\n",
    "                     'RMSE on train data': [rmse_train], 'RMSE on test data': [rmse_test]}\n",
    "    estimates_df = pd.DataFrame(estimates_dict, index = ['Metric Value'])\n",
    "    return estimates_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lre_multiple = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_eval(lre_multiple, x_train, x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, multiple linear regression has a much lower score when we use more features instead of just one as this exposes more information to our linear regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 3: Polynomial Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we simply use a pipeline instead of our 'train_and_eval' function. Since the polynomial regression object has no .fit() method which is within our defined function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Input=[('scale',StandardScaler()), ('polynomial', PolynomialFeatures(degree=3, include_bias=False)), ('model',LinearRegression())]\n",
    "pipe=Pipeline(Input)\n",
    "pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our polynomial features, we cannot use all the dummy variable features, as this requires 329GiB / 353,261 GB, because with polynomial features we have cross terms. In our case, we have a cross term that's an array with shape (1370, 32224113) and data type float64. So we will just use `'low_price`, `high_price`, `month` and `price_kg`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(x_train[['low_price', 'high_price', 'month', 'price_kg']], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction on train\n",
    "ypipe_train=pipe.predict(x_train[['low_price', 'high_price', 'month', 'price_kg']])\n",
    "ypipe_train[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction on test\n",
    "ypipe_test=pipe.predict(x_test[['low_price', 'high_price', 'month', 'price_kg']])\n",
    "ypipe_test[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_dict = {'R^2 on train data': [r2_score(y_train, ypipe_train)], 'R^2 on test data': [r2_score(y_test, ypipe_test)],\n",
    "                     'RMSE on train data': [math.sqrt(mean_squared_error(y_train , ypipe_train))], \n",
    "                'RMSE on test data': [math.sqrt(mean_squared_error(y_test, ypipe_test))]}\n",
    "metrics_df = pd.DataFrame(metrics_dict, index = ['Metric Value'])\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The polynomial regression model using a polynomial of order 3 and using only the four features that we gave it doesn't do any much better than multiple linear regression on test data.\n",
    "\n",
    "We can make a plot to see if we have used the best order for our polynomial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_test = []\n",
    "rsqd_test = []\n",
    "order = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "for n in order:\n",
    "    Input=[('scale',StandardScaler()), ('polynomial', PolynomialFeatures(degree=n, include_bias=False)), ('model',LinearRegression())]\n",
    "    pipe=Pipeline(Input)\n",
    "    pipe.fit(x_train[['low_price', 'high_price', 'month', 'price_kg']], y_train)\n",
    "    ypipe_test = pipe.predict(x_test[['low_price', 'high_price', 'month', 'price_kg']])\n",
    "    rmse_test.append(math.sqrt(mean_squared_error(y_test , ypipe_test)))\n",
    "    rsqd_test.append(r2_score(y_test, ypipe_test))\n",
    "\n",
    "# Create figure and axes\n",
    "f, ax = plt.subplots(figsize=(15,5), nrows=1, ncols=2, sharey=False)\n",
    "\n",
    "# Create list of titles\n",
    "title = ['RMSE Using Test Data','R^2 Using Test Data']\n",
    "\n",
    "# Plot the subplots\n",
    "ax[0].set_ylim([0, 5])\n",
    "ax[0].set_xlabel('order')\n",
    "ax[0].set_ylabel('RMSE')\n",
    "ax[0].plot(order, rmse_test)\n",
    "ax[0].set_title(title[0])\n",
    "\n",
    "ax[1].set_ylim([-1, 1])\n",
    "ax[1].set_xlabel('order')\n",
    "ax[1].set_ylabel('R^2')\n",
    "ax[1].plot(order, rsqd_test)\n",
    "ax[1].set_title(title[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly a polynomial of **degree three** will minimize the RMSE on test data and maximize the R$^{2}$ as well. We use these graphs to find the optimal parameters to use for PolynomialFeatures. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 4: Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtr= DecisionTreeRegressor(max_depth=5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_eval(dtr, x_train, x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision tree alone doesn't do any better than the multiple linear regression model on test data. We can however use random forests, which is an ensemble learning method that may significantly improve our results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 5: Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the Random Forest model and see how the RMSE on test data is like. We will use the same max_depth number as we used on the decision tree above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(n_estimators=500, max_depth=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_eval(rf, x_train, x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the random forest mode has a much lower RMSE on test data than a single decision tree, simply by averaging the predictions of many decision trees. Also it turns out that our random forest model has the lowest RMSE on test data so far. Furtheremore, this RMSE can be further lowered by hyperparameter tunning. Which we will do in the next section. See the subsection called **Grid Search**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 6: Support Vector Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sv_reg = SVR(kernel='rbf', gamma='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_eval(sv_reg, x_train, x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The support vector regressor doesn't do well enough on the test data compared to the random forest regressor. This may be because the random forest regressor is an ensemble learning model and the suport vector regressor is not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 7: XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost is the most accurate modelling technique for structured data. This is especially true when we carefully tune it's parameters.\n",
    "\n",
    "**Gradient boosting** is an ensemble method. It is a type of machine learning boosting. It relies on the intuition that the best possible next model, when combined with previous models, minimizes the overall prediction error. The key idea is to set the target outcomes for this next model in order to minimize the error. In tree boosting, each new model that is added to the ensemble is a decision tree. XGBoost provides parallel tree boosting (also known as GBDT, GBM) that solves many data science problems in a fast and accurate way. For many problems, XGBoost is one of the best gradient boosting machine (GBM) frameworks today.\n",
    "\n",
    "We first have to initialize our ensemble model with a single model (even if the predictions are very innacurate, the later additions of models into the ensemble will minimize those errors for us.\n",
    "\n",
    "After initializing our ensemble model with a single model, we start the cycle:\n",
    "\n",
    "- First, we use the current ensemble to generate predictions for each observation in the dataset. To make a prediction, we add the predictions from all models in the ensemble.\n",
    "- These predictions are used to calculate a loss function (like the mean squared error, for instance).\n",
    "- Then, we use the loss function to fit a new model that will be added to the ensemble. Specifically, we determine model parameters so that adding this new model to the ensemble will reduce the loss. (Side note: The \"gradient\" in \"gradient boosting\" refers to the fact that we'll use gradient descent on the loss function to determine the parameters in this new model.)\n",
    "- Finally, we add the new model to ensemble, and ... repeat!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize ensemble with a single model\n",
    "my_model = XGBRegressor()\n",
    "my_model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are descriptions of the parameters of the XGBRegressor:\n",
    "- `n_estimators` specifies how many times to go through the modeling cycle described above. It is equal to the number of models that we include in the ensemble.\n",
    "- `early_stopping_rounds` offers a way to automatically find the ideal value for n_estimators. Early stopping causes the model to stop iterating when the validation score stops improving. When using `early_stopping_rounds`, you also need to set aside some data for calculating the validation scores - this is done by setting the `eval_set` parameter.\n",
    "- `learn_rate`specifies the learning rate by which to shrink the feature weights. Shrinking feature weights after each boosting step makes the boosting process more conservative and prevents overfitting. The range is 0.0 to 1.0. This value defaults to 0.3. In general, a small learning rate and large number of estimators will yield more accurate XGBoost models, though it will also take the model longer to train since it does more iterations through the cycle.\n",
    "- `objective`: This defines the loss function to be minimized.\n",
    "\n",
    "For more info on other parameters, see [XGBoost paarmeters](https://xgboost.readthedocs.io/en/latest/parameter.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate prediction with the single model ensemble\n",
    "predictions = my_model.predict(x_test)\n",
    "print(\"Mean Squared Error: \" + str(mean_squared_error(predictions, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit in new models to the ensemble\n",
    "my_model = XGBRegressor(n_estimators=1000, learning_rate=0.05)\n",
    "my_model.fit(x_train, y_train, \n",
    "             early_stopping_rounds=5, \n",
    "             eval_set=[(x_test, y_test)],\n",
    "             verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = my_model.predict(x_test)\n",
    "print(\"Mean Squared Error: \" + str(mean_squared_error(predictions, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RMSE of the test data with XGBoost may further be lowered with the help of hyperparameter tunning as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensemble Learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models = [(\"LR\",lre),(\"DT\",dtr),(\"SVR\",sv_reg)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_weightings = np.array([0.1,0.3,0.6])\n",
    "v_reg = VotingRegressor(estimators=models,weights=model_weightings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we have built a voting-based ensemble that combines:\n",
    "- 10% of Linear Regression model output\n",
    "- 30% of Decision Tree output\n",
    "- 60% of Support Vector Regression output\n",
    "\n",
    "to make a final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_reg.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_eval(v_reg, x_train, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dictionary of results\n",
    "results_dict = {'Training MSE':\n",
    "                    {\n",
    "                        \"Linear Regression\": 1,\n",
    "                        \"Multiple Linear Regression\": 2,\n",
    "                        \"Polynomial Regression\": 3,\n",
    "                        \"Decision Tree\": 4,\n",
    "                        \"Support Vector Regressor\": 5,\n",
    "                        \"Random Forest\": 6                       \n",
    "                    },\n",
    "                'Test MSE':\n",
    "                    {\n",
    "                        \"Linear Regression\": 7,\n",
    "                        \"Multiple Linear Regression\": 8,\n",
    "                        \"Polynomial Regression\": 9,\n",
    "                        \"Decision Tree\": 10,\n",
    "                        \"Support Vector Regressor\": 11,\n",
    "                        \"Random Forest\": 12  \n",
    "                    },\n",
    "                'Test RMSE':\n",
    "                    {\n",
    "                        \"Linear Regression\": 13,\n",
    "                        \"Multiple Linear Regression\": 14,\n",
    "                        \"Polynomial Regression\": 15,\n",
    "                        \"Decision Tree\": 16,\n",
    "                        \"Support Vector Regressor\": 17,\n",
    "                        \"Random Forest\": 18  \n",
    "                    }\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results_dict)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"modelselection\">Model Selection, Overfitting and Underfitting  </h1>\n",
    "<p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pr=PolynomialFeatures(degree=3)\n",
    "x_train_pr = pr.fit_transform(x_train[['low_price', 'high_price', 'month', 'price_kg']])\n",
    "x_test_pr = pr.fit_transform(x_test[['low_price', 'high_price', 'month', 'price_kg']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RidgeModel=Ridge(alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RidgeModel.fit(x_train_pr, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = RidgeModel.predict(x_test_pr)\n",
    "yhat[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The first 5 predicted values: ', yhat[0:5], '\\n The actual/test values are ', y_test[0:5].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Rsqu_test = []\n",
    "Rsqu_train = []\n",
    "dummy1 = []\n",
    "alpha_params = 10 * np.array(range(0,1000))\n",
    "for alfa in alpha_params:\n",
    "    RigeModel = Ridge(alpha=alfa) \n",
    "    RigeModel.fit(x_train_pr, y_train)\n",
    "    Rsqu_test.append(RigeModel.score(x_test_pr, y_test))\n",
    "    Rsqu_train.append(RigeModel.score(x_train_pr, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 12\n",
    "height = 10\n",
    "plt.figure(figsize=(width, height))\n",
    "\n",
    "plt.plot(alpha_params,Rsqu_test, label='validation data  ')\n",
    "plt.plot(alpha_params,Rsqu_train, 'r', label='training Data ')\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('R^2')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_params1= [{'alpha': [0.001,0.1,1, 10, 100, 10000, 100000, 100000]}]\n",
    "alpha_params1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr = Ridge()\n",
    "rr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid_rr = GridSearchCV(rr, alpha_params1, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_rr.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BestRR=grid_rr.best_estimator_\n",
    "BestRR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "math.sqrt(mean_squared_error(y_test, BestRR.predict(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use all of the data instead of the train and validation set data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the parameter grid based on the results of random search \n",
    "param_grid = {\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [5, 10, 20, 40, 70, 90],\n",
    "    'max_features': [2, 3],\n",
    "    'min_samples_leaf': [3, 4],\n",
    "    'min_samples_split': [4, 6, 8, 10],\n",
    "    'n_estimators': [200, 300, 400, 500]\n",
    "}\n",
    "# Create a based model\n",
    "rf = RandomForestRegressor()\n",
    "# Instantiate the grid search model\n",
    "grid_rf = GridSearchCV(estimator = rf, param_grid = param_grid, \n",
    "                          cv = 5, n_jobs = -1, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_rf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BestRF = grid_rf.best_estimator_\n",
    "BestRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# RMSE on test data for random forest\n",
    "math.sqrt(mean_squared_error(y_test, BestRF.predict(x_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a brief description of other relevant parameters of XGBoost that are not yet explained in this notebook:\n",
    "- booster: Select the type of model to run at each iteration\n",
    "    gbtree: tree-based models\n",
    "    gblinear: linear models\n",
    "- nthread: default to maximum number of threads available if not set\n",
    "- objective: This defines the loss function to be minimized\n",
    "\n",
    "Parameters that control the computation speed\n",
    "\n",
    "- subsample: Denotes the fraction of observations to be randomly samples for each tree\n",
    "- colsample_bytree: Subsample ratio of columns when constructing each tree.\n",
    "- n_estimators: Number of trees to fit.\n",
    "\n",
    "Parameters which control the bias and variance trade-off (underfitting and overfitting). \n",
    "\n",
    "- learning_rate: Makes the model more robust by shrinking the weights on each step\n",
    "- max_depth: The maximum depth of a tree.\n",
    "- min_child_weight: Defines the minimum sum of weights of all observations required in a child."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#XGBoost hyper-parameter tuning\n",
    "param_tuning = {\n",
    "        'booster': ['gbtree'],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'min_child_weight': [1, 3, 5],\n",
    "        'subsample': [0.5, 0.7, 1],\n",
    "        'colsample_bytree': [0.5, 0.7],\n",
    "        'n_estimators' : [100, 200, 500, 1000],\n",
    "        'objective': ['reg:squarederror'],\n",
    "        'eval_set': [(x_test, y_test)],\n",
    "        'early_stopping_rounds': [5]\n",
    "    }\n",
    "xgb_model = XGBRegressor()\n",
    "\n",
    "grid_xgb = GridSearchCV(estimator = xgb_model,\n",
    "                           param_grid = param_tuning,                        \n",
    "                           scoring = 'neg_mean_squared_error',  #MSE\n",
    "                           cv = 5,\n",
    "                           n_jobs = -1,\n",
    "                           verbose = 1)\n",
    "\n",
    "grid_xgb.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_XGB = grid_xgb.best_estimator_\n",
    "best_XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# rmse of test data for XGBoost\n",
    "math.sqrt(mean_squared_error(y_test, best_XGB.predict(x_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the XGBoost model out performs all the other models after hyperparameter tunnning; it has the lowest RSME score on the test data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Conclusion_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The conclusion here is that the best model to use is XGBoost, since it has the best result on the desired metric on test data after careful parameter tunning, i.e XGBoost gives the lowest RMSE on test data. In addition, we saw that it gave us the best $R^{2}$ value, i.e it gave us the highest $R^{2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will make some predictions on the actual test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 685 observations and 13 features just as in the original train dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As, expected, there are no missing values in this test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#fixing the 'Date' column in the test dataset\n",
    "test['Date'] = pd.to_datetime(test['Date'], format=\"%Y.%m.%d\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Date` column is not in the correct format and we need to change it to the `datetime` format just as we did for the train dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dummy_test = pd.get_dummies(test[['Province', 'Size_Grade', 'Container']])\n",
    "test = pd.concat([test, dummy_test], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also had to create the dummy variables for the test set, because our regression models understand numbers not words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test['price_kg'] = test['Sales_Total'] * test['Weight_Kg'] / test['Total_Qty_Sold']  \n",
    "test['month'] = test['Date'].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Drop non-numerical columns\n",
    "\n",
    "test.drop(columns=['Province', 'Size_Grade', 'Container', 'Commodities', 'Date'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a column that's not in the train dataframe after using get dummies. The column is `'Container_IA00'`. A for-loop may be used to detect other missing columns, but after making the column names lower cases and this lowering of cases has to happen before using get dummies on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test.drop(columns=['Container_IA400'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model = Ridge(alpha=0.0005, copy_X=True, fit_intercept=True, max_iter=None,\n",
    "#      normalize=False, random_state=None, solver='auto', tol=0.001)\n",
    "model = XGBRegressor(n_estimators=1000, learning_rate=0.05)\n",
    "model.fit(train_x, train_y)\n",
    "                \n",
    "preds = model.predict(test)\n",
    "preds[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import the model as a pickle file for the API to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "model_save_path = \"model.pkl\"\n",
    "with open(model_save_path,'wb') as file:\n",
    "    pickle.dump(model,file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index = test['Index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create submission dataframe\n",
    "submission = pd.DataFrame(\n",
    "    {'Index': index,\n",
    "     'avg_price_per_kg': preds\n",
    "    })\n",
    "\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Save DataFrame to csv file for submission\n",
    "submission.to_csv(\"19th_submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END OF NOTEBOOK"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
